{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code : cas général.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO2oL8Usz1k3knz6fe/YG3A"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"COhnUctCajyy"},"source":["### Fonctions d'activation\r\n","Nous allons commencer par définir les fonctions d'activations ainsi que leurs dérivées"]},{"cell_type":"code","metadata":{"id":"LHno8y6raYWw"},"source":["import numpy as np\r\n","import random\r\n","\r\n","def sigmoid(t):\r\n","    # Fonction d'activation sigmoïde\r\n","    return 1 / (1 + np.exp(-t))\r\n","\r\n","def sigmoid_prime(t):\r\n","    # Dérivée de la fonction sigmoïde\r\n","    return sigmoid(t) * (1 - sigmoid(t))\r\n","\r\n","def relu(t):\r\n","    # Fonction Relu\r\n","    return np.maximum(0,t)\r\n","\r\n","def relu_prime(t):\r\n","    for i in range(len(t)):\r\n","        if t[i]>0 : t[i] = 1\r\n","        else: t[i] = 0 # on considère que pour t=0, relu'(t)=0\r\n","    return t\r\n","    \r\n","def identite(t):\r\n","    return t\r\n","\r\n","def identite_prime(t):\r\n","    return 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JlJRBY0Xa47A"},"source":["### Classe Réseau de neurones\r\n","Nous allons ici créer la classe ReseauNeurones"]},{"cell_type":"code","metadata":{"id":"j94Xa4EIaz_y"},"source":["\"\"\"\r\n","    Entrée : la taille du vecteur d'entrée\r\n","    Sortie : une instance de réseau de neurones\r\n","\"\"\"\r\n","\r\n","class ReseauNeurones(object):\r\n","\r\n","    def __init__(self, taille):\r\n","        # self.taille est la taille du vecteur d'entrée\r\n","        # Dans chaque couche, on génère un nombre aléatoire de neurones\r\n","    \r\n","        self.a0 = np.array([[]]) # Initialisation de l'activité a0 par un tableau vide\r\n","        self.y = 0.0\r\n","        # Vecteur de poids et biais de la couche 1\r\n","        self.w1 = np.random.randn(random.randint(2, 5), taille)\r\n","        self.b1 = np.random.randn(self.w1.shape[0], 1)\r\n","        self.z1, self.a1 = np.array([[]]), np.array([[]]) #initialisation de l'activité\r\n","        \r\n","        # Vecteur de poids et biais de la couche 2\r\n","        self.w2 = np.random.randn(random.randint(2, 4), self.w1.shape[0]) # self.w1.shape[0] renvoie le nombre de ligne de w1\r\n","        self.b2 = np.random.randn(self.w2.shape[0], 1) # self.w1.shape[0] renvoie le nombre de ligne de w1\r\n","        self.z2, self.a2 = np.array([[]]), np.array([[]])\r\n","        \r\n","        # Vecteur de poids et biais de la couche 3\r\n","        self.w3 = np.random.randn(random.randint(2, 3), self.w2.shape[0]) ## self.taille - 3 est un choix arbitraire\r\n","        self.b3 = np.random.randn(self.w3.shape[0], 1)\r\n","        self.z3, self.a3 = np.array([[]]), np.array([[]])\r\n","        \r\n","        # Dernière couche\r\n","        self.w4 = np.random.randn(1, self.w3.shape[0]) # un seul neurone\r\n","        self.b4 = np.random.randn(self.w4.shape[0], 1)\r\n","        self.z4, self.a4 = np.array([[]]), np.array([[]])\r\n","        \r\n","        # Ces varibales nous seront utiles pour la suite\r\n","        # On augmente les dimensions des tableaux pour pouvoir utiliser la formule de recurrence de la prop. avant\r\n","        self.neurones = [np.array([[]]), self.w1, self.w2, self.w3, self.w4]\r\n","        self.biais = [np.array([[]]), self.b1, self.b2, self.b3, self.b4]\r\n","        self.preac = [np.array([[]]), self.z1, self.z2, self.z3, self.z4]\r\n","        self.activ = [self.a0, self.a1, self.a2, self.a3, self.a4]\r\n","        \r\n","        self.gw1, self.gw2, self.gw3, self.gw4 = np.array([[]]), np.array([[]]), np.array([[]]), np.array([[]])\r\n","        self.gb1, self.gb2, self.gb3, self.gb4 = np.array([[]]), np.array([[]]), np.array([[]]), np.array([[]])\r\n","        \r\n","        self.grad_w = [np.array([[]]), self.gw1, self.gw2, self.gw3, self.gw4]\r\n","        self.grad_b = [np.array([[]]), self.gb1, self.gb2, self.gb3, self.gb4]\r\n","        \r\n","        #print(self.biais) \r\n","\r\n","    def propagation_avant(self, X):\r\n","        # Propagation avant\r\n","        self.activ[0] = X # a0 = X // initialisation\r\n","        \r\n","        for i in range(1, len(self.neurones)-1):\r\n","            self.preac[i] = self.neurones[i].dot(self.activ[i-1])\r\n","            self.activ[i] = relu(self.preac[i]) #on a decidé d'utiliser la fonction relu\r\n","        \r\n","        # Dernière couche\r\n","        # l'indice -1 permet de recupérer le dernier élément d'un tableau\r\n","        self.preac[-1] = self.neurones[-1].dot(self.activ[-2])\r\n","        self.activ[-1] = sigmoid(self.preac[-1]) # on décide de faire de la classification 0/1\r\n","        #print(self.preac[-1], '\\n\\n', self.activ[-1]) # c'était un test\r\n","        \r\n","        self.y = float(self.activ[-1])  # Prédiction finale\r\n","        print('La prédiction est : {}'.format(self.y))\r\n","        return self.y\r\n","\r\n","    def retropropagation(self, X, cible, taux=1):\r\n","        # Retropropagation du coût\r\n","        tmp = self.propagation_avant(X) # On récupère la prédiction de la propagation avant\r\n","        \r\n","        ga = 2*(tmp - cible) # Initialisation du gradient par rapport à y = a^L\r\n","        gz = ga * sigmoid_prime(self.preac[-1])\r\n","        self.grad_w[-1] = gz.dot(self.activ[-2].T) * taux\r\n","        self.grad_b[-1] = gz * taux\r\n","        \r\n","        ga = self.neurones[-1].T.dot(gz)\r\n","        \r\n","        for j in range(len(self.neurones)-2, 0, -1):\r\n","        \r\n","            gz = ga * relu_prime(self.preac[j])\r\n","            self.grad_w[j] = gz.dot(self.activ[j-1].T) * taux\r\n","            self.grad_b[j] = gz * taux\r\n","            ga = self.neurones[j].T.dot(gz)\r\n","\r\n","    def pas_gradient(self, X, cible, etapes, taux):\r\n","\r\n","        for etape in range(etapes):\r\n","            \r\n","            # On effectue une rétropropagation et on récupère les dérivées\r\n","            self.retropropagation(X, cible, taux)\r\n","            \r\n","            # Mise à jour des W^i et des b^i\r\n","            for k in range(len(self.neurones)-1, 0, -1):\r\n","                self.neurones[k] = self.neurones[k] - self.grad_w[k]\r\n","                self.biais[k] = self.biais[k] - self.grad_b[k]\r\n","            \r\n","    def ajouter_couche(self, nb_neurones):\r\n","        # On ajoute la nouvelle couche à la fin, avant la dernière couche de prédiction\r\n","        \r\n","        dim = self.neurones[-2].shape[0] # On récupère le nbre de lignes de l'avant dernière matrice\r\n","        self.neurones.insert(-1, np.random.randn(nb_neurones, dim))\r\n","        self.biais.insert(-1, np.random.randn(nb_neurones, 1))\r\n","        \r\n","        # Mise à jour de la dernière couche\r\n","        self.neurones[-1] = np.random.randn(1, nb_neurones)\r\n","        return \"Nouvelle couche ajoutée avec succès !\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z2GhX56RbZS3"},"source":["## Vous pouvez tester les différentes fonctions"]},{"cell_type":"code","metadata":{"id":"3JHCFwpgbYBC"},"source":["#here"],"execution_count":null,"outputs":[]}]}